"""
KL divergence assumes likelihood to be equal to the prior.
by minimizing KL divergence, we are minmizing the difference between the posterior and prior, thus maximizing posterior
this class assumes the model output to follow Gaussian distribution
"""