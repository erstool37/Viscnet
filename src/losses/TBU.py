"""
KL divergence assumes likelihood to be equal to the prior.
by minimizing KL divergence, we are minmizing the difference between the posterior and prior, thus maximizing posterior
this class assumes the model output to follow Gaussian distribution
"""



"""
for GMM(Gausian Mixture Model),
the model outputs one value, mixed with varying parameters(viscosity, density, surface tension)
assuming each parameter to follow Gaussian distribution, we split the output based on GMM
possibly adding physical information to split the model may be valid
"""


"""
MAP: maximum a posteriori via to loss function,
model output assumed to follow Gaussian Distribution
prior calculated based on the trian data..?
"""