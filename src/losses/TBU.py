"""
KL divergence assumes likelihood to be equal to the prior.
KL divergence = CE - H
CE: cross entropy, H: entropy
"""



"""
for GMM(Gausian Mixture Model),
the model outputs one value, mixed with varying parameters(viscosity, density, surface tension)
assuming each parameter to follow Gaussian distribution, we split the output based on GMM
possibly adding physical information to split the model may be valid
"""


"""
MAP: maximum a posteriori via to loss function,
model output assumed to follow Gaussian Distribution
prior calculated based on the trian data..?
"""